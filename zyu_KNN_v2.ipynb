{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W207 Forest Cover - KNN v2\n",
    "\n",
    "## Plan\n",
    "\n",
    "#### Feature Engineering\n",
    "\n",
    "- More data cleaning, removing useless data columns\n",
    "\n",
    "- Feature engineering: using only the most important features\n",
    "\n",
    "- Feature engineering: combine some of the soil types\n",
    "\n",
    "#### Error Analysis\n",
    "\n",
    "- Confusion matrix: find out which types give the most problems\n",
    "\n",
    "- Any clue from EDA?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('train.csv')             # read training data\n",
    "data_test = pd.read_csv('test.csv')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(data_train, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'Elevation', 'Aspect', 'Slope',\n",
       "       'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n",
       "       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n",
       "       'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points',\n",
       "       'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n",
       "       'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n",
       "       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n",
       "       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n",
       "       'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n",
       "       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n",
       "       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n",
       "       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n",
       "       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n",
       "       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n",
       "       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40',\n",
       "       'Cover_Type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to keep track of things\n",
    "target = df_train.columns[-1]\n",
    "all_features_ALL = df_train.columns[1:-1]\n",
    "\n",
    "num_features = df_train.columns[1:11]\n",
    "cat_features_ALL = df_train.columns[11:-1]\n",
    "\n",
    "wild_features = df_train.columns[11:15]\n",
    "soil_features_ALL = df_train.columns[15:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove constant Soil_Type7 and Soil_Type15\n",
    "all_features = all_features_ALL.drop(['Soil_Type7', 'Soil_Type15'])\n",
    "cat_features = cat_features_ALL.drop(['Soil_Type7', 'Soil_Type15'])\n",
    "soil_features = soil_features_ALL.drop(['Soil_Type7', 'Soil_Type15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top17_features = all_features_ALL[[0,5,9,13,3,4,6,1,8,7,23,51,16,2,52,10,12]]\n",
    "\n",
    "top10_features = all_features_ALL[[0,5,9,13,3,4,6,1,8,7]]\n",
    "\n",
    "top5_features = all_features_ALL[[0,5,9,13,3]]\n",
    "\n",
    "top2_features = all_features_ALL[[0,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Elevation', 'Horizontal_Distance_To_Roadways',\n",
       "       'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area4',\n",
       "       'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n",
       "       'Hillshade_9am', 'Aspect', 'Hillshade_3pm', 'Hillshade_Noon',\n",
       "       'Soil_Type10', 'Soil_Type38', 'Soil_Type3', 'Slope', 'Soil_Type39',\n",
       "       'Wilderness_Area1', 'Wilderness_Area3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top17_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. KNN Pipeline\n",
    "\n",
    "- input a list of features\n",
    "\n",
    "- separate features into numeric and categorical\n",
    "\n",
    "- using StandardScaling on the numeric features\n",
    "\n",
    "- combine numeric and categorical features\n",
    "\n",
    "- PCA optional\n",
    "\n",
    "- fit KNN\n",
    "\n",
    "### Note:\n",
    "\n",
    "- Adding MinMaxScaler and Normalizer doesn't seem to help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
       "       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
       "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
       "       'Horizontal_Distance_To_Fire_Points'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = top17_features\n",
    "\n",
    "num_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, Imputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# STEP 1: Define the features  #########################################\n",
    "def get_num_features(features):\n",
    "    num_features = [item for item in features if 'Soil' not in item and 'Wilderness' not in item ]\n",
    "    return num_features\n",
    "\n",
    "def get_cat_features(features):\n",
    "    cat_features = [item for item in features if 'Soil' in item or 'Wilderness' in item ]\n",
    "    return cat_features\n",
    "\n",
    "features = all_features\n",
    "\n",
    "num_features = get_num_features(features)\n",
    "cat_features = get_cat_features(features)\n",
    "\n",
    "# STEP 2: Define the KNN pipeline ######################################\n",
    "\n",
    "def select_num_features(X):\n",
    "    return X[num_features]\n",
    "\n",
    "def select_cat_features(X):\n",
    "    return X[cat_features]\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "num_feature_pipeline = Pipeline(steps=[\n",
    "    ('select', FunctionTransformer(select_num_features, validate=False)),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_feature_pipeline = Pipeline(steps=[\n",
    "    ('select', FunctionTransformer(select_cat_features, validate=False) )    \n",
    "])\n",
    "\n",
    "fu = FeatureUnion([\n",
    "    ('numeric', num_feature_pipeline),\n",
    "    ('categorical', cat_feature_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_num = 1\n",
    "\n",
    "knn_pipe = Pipeline(steps=[\n",
    "    ('preprocess', fu),\n",
    "    ('predict', KNeighborsClassifier(n_neighbors=knn_num))\n",
    "])\n",
    "\n",
    "# knn_pipe.fit(df_train, df_train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STEP 3: Train the KNN_pipe and Make predictions ########################\n",
    "knn_pipe.fit(data_train, data_train[target])\n",
    "\n",
    "pred = knn_pipe.predict(data_test)\n",
    "ID = data_test.Id\n",
    "\n",
    "# STEP 4: Write to file for Kaggle Submission #############################\n",
    "data_out = []\n",
    "for i in range(len(ID)):\n",
    "    data_out.append({'ID':ID.iloc[i], 'Cover_Type':pred[i]})\n",
    "\n",
    "import csv\n",
    "with open('test_submission_top2_knn_' + str(knn_num) + '.csv', 'wt') as fout:\n",
    "    cout = csv.DictWriter(fout, ['ID','Cover_Type'])\n",
    "    cout.writeheader()\n",
    "    cout.writerows(data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Scores of KNN Pipeline on Kaggle \n",
    "\n",
    "Use all_features, top17_features, top10_features, top5_features, top2_features and test the above pipeline with knn=1,2,5,10 and score on Kaggle, following are the results:\n",
    "\n",
    "### all_features\n",
    "\n",
    "- knn = 1: 0.6906\n",
    "\n",
    "- knn = 2: 0.68584\n",
    "\n",
    "- knn = 5: 0.64546\n",
    "\n",
    "- knn = 10: 0.61504\n",
    "\n",
    "\n",
    "### top17_features\n",
    "\n",
    "- knn = 1: 0.65998\n",
    "\n",
    "- knn = 2: 0.66320\n",
    "\n",
    "- knn = 5: 0.62491\n",
    "\n",
    "- knn = 10: 0.59760\n",
    "\n",
    "\n",
    "### top10_features\n",
    "\n",
    "- knn = 1: 0.63003\n",
    "\n",
    "- knn = 2: 0.63973\n",
    "\n",
    "- knn = 5: 0.59013\n",
    "\n",
    "- knn = 10: 0.55603\n",
    "\n",
    "\n",
    "### top5_features\n",
    "\n",
    "- knn = 1: 0.65710\n",
    "\n",
    "- knn = 2: 0.65603\n",
    "\n",
    "- knn = 5: 0.61171\n",
    "\n",
    "- knn = 10: 0.58272\n",
    "\n",
    "\n",
    "### top2_features\n",
    "\n",
    "- knn = 1: 0.48068\n",
    "\n",
    "- knn = 2: 0.52666\n",
    "\n",
    "- knn = 5: 0.50603\n",
    "\n",
    "- knn = 10: 0.50100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "\n",
    "#### ROC curve is not applicable in this case, it is strictly restricted to binary cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = df_test[target]                # get correct labels\n",
    "y_pred_knn = knn_pipe.predict(df_test)  # get predicted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00       552\n",
      "          2       1.00      1.00      1.00       561\n",
      "          3       1.00      1.00      1.00       530\n",
      "          4       1.00      1.00      1.00       563\n",
      "          5       1.00      1.00      1.00       513\n",
      "          6       1.00      1.00      1.00       529\n",
      "          7       1.00      1.00      1.00       532\n",
      "\n",
      "avg / total       1.00      1.00      1.00      3780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "F1_score_test = f1_score(y_test, y_pred_knn, average='weighted').round(4)\n",
    "report_knn = classification_report(y_test, y_pred_knn)\n",
    "\n",
    "# print(F1_score_test)\n",
    "print(report_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cross-validation score\n",
    "\n",
    "We would like to get a better estimate of the performance without contaminating the test data. Try using cross-validation to estimate the performance.\n",
    "\n",
    "(The results seem similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7878 0.8075 0.7932 0.7771 0.8009]\n",
      "0.7933\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores_knn = cross_val_score(knn_pipe, df_train, df_train[target], cv=5)\n",
    "print(scores_knn.round(4))\n",
    "print(scores_knn.mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[552   0   0   0   0   0   0]\n",
      " [  0 561   0   0   0   0   0]\n",
      " [  0   0 530   0   0   0   0]\n",
      " [  0   0   0 563   0   0   0]\n",
      " [  0   0   0   0 513   0   0]\n",
      " [  0   0   0   0   0 529   0]\n",
      " [  0   0   0   0   0   0 532]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "\n",
    "confusion_knn = confusion_matrix(y_test, y_pred_knn, labels=[1,2,3,4,5,6,7])\n",
    "print(confusion_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commnets:\n",
    "\n",
    "- Cover_Types 4 and 7 have the best performance, from EDA, Cover_Type4 is the one live at the lowest elevation, Cover_Type7 is the one live at the highest elevation;\n",
    "\n",
    "- There is a lot of confusing between 1 and 2: they live in similar elevations just below 7, there is also a lot of confusion between 1 and 7;\n",
    "\n",
    "- Confusion between 6 and 3, 6 and 4 can also be explained by the closeness in elevation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Logistic Regression Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_pipe = Pipeline(steps=[\n",
    "    ('preprocess', fu),\n",
    "    ('predict', LogisticRegression(penalty='l2', C=2.0))\n",
    "])\n",
    "\n",
    "lr_pipe.fit(df_train, df_train[target])\n",
    "\n",
    "y_pred_lr = lr_pipe.predict(df_test)  # get predicted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.63      0.63      0.63       552\n",
      "          2       0.58      0.48      0.53       561\n",
      "          3       0.61      0.56      0.59       530\n",
      "          4       0.82      0.89      0.85       563\n",
      "          5       0.61      0.70      0.65       513\n",
      "          6       0.58      0.59      0.59       529\n",
      "          7       0.87      0.86      0.87       532\n",
      "\n",
      "avg / total       0.67      0.68      0.67      3780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_lr = classification_report(y_test, y_pred_lr)\n",
    "print(report_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cross-validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6693 0.6789 0.6755 0.6708 0.6614]\n",
      "0.6712\n"
     ]
    }
   ],
   "source": [
    "scores_lr = cross_val_score(lr_pipe, df_train, df_train[target], cv=5)\n",
    "print(scores_lr.round(4))\n",
    "print(scores_lr.mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[349  96   1   0  36   6  64]\n",
      " [128 269   9   0 126  24   5]\n",
      " [  0   2 297  71  15 145   0]\n",
      " [  0   0  30 503   0  30   0]\n",
      " [ 11  65  51   0 361  25   0]\n",
      " [  0  25  97  42  51 314   0]\n",
      " [ 66   5   0   0   2   0 459]]\n"
     ]
    }
   ],
   "source": [
    "confusion_lr = confusion_matrix(y_test, y_pred_lr, labels=[1,2,3,4,5,6,7])\n",
    "print(confusion_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check LR score on Kaggle\n",
    "\n",
    "- all_features: 0.67 on test, 0.5603 on Kaggle\n",
    "\n",
    "- top17_features: 0.62 on test, 0.4916 on Kaggle\n",
    "\n",
    "- top10_features: 0.58 on test, 0.45328 on Kaggle\n",
    "\n",
    "- top5_features: 0.53 on test, 0.42169 on Kaggle\n",
    "\n",
    "- top2_features: 0.51 on test, 0.39383 on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STEP 3: Train the KNN_pipe and Make predictions ########################\n",
    "lr_pipe.fit(data_train, data_train[target])\n",
    "\n",
    "pred = lr_pipe.predict(data_test)\n",
    "ID = data_test.Id\n",
    "\n",
    "# STEP 4: Write to file for Kaggle Submission #############################\n",
    "data_out = []\n",
    "for i in range(len(ID)):\n",
    "    data_out.append({'ID':ID.iloc[i], 'Cover_Type':pred[i]})\n",
    "\n",
    "import csv\n",
    "with open('test_submission_top5_lr_pipe.csv', 'wt') as fout:\n",
    "    cout = csv.DictWriter(fout, ['ID','Cover_Type'])\n",
    "    cout.writeheader()\n",
    "    cout.writerows(data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Forest Pipeline\n",
    "\n",
    "### Note:\n",
    "\n",
    "- increase the RF n_estimators from 100 to 500 improves F1 score by 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_pipe = Pipeline(steps=[\n",
    "    ('preprocess', fu),\n",
    "    ('predict', RandomForestClassifier(n_estimators=500, n_jobs=-1, oob_score=True, random_state=121))\n",
    "])\n",
    "\n",
    "rf_pipe.fit(df_train, df_train[target])\n",
    "\n",
    "y_pred_rf = rf_pipe.predict(df_test)  # get predicted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.78      0.74      0.76       552\n",
      "          2       0.79      0.68      0.73       561\n",
      "          3       0.86      0.83      0.85       530\n",
      "          4       0.94      0.97      0.96       563\n",
      "          5       0.88      0.94      0.91       513\n",
      "          6       0.85      0.90      0.87       529\n",
      "          7       0.91      0.96      0.93       532\n",
      "\n",
      "avg / total       0.86      0.86      0.86      3780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_rf = classification_report(y_test, y_pred_rf)\n",
    "print(report_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cross-validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8432 0.8533 0.862  0.8522 0.8455]\n",
      "0.8512\n"
     ]
    }
   ],
   "source": [
    "scores_rf = cross_val_score(rf_pipe, df_train, df_train[target], cv=5)\n",
    "print(scores_rf.round(4))\n",
    "print(scores_rf.mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[410  79   0   0  10   3  50]\n",
      " [101 379  12   0  49  17   3]\n",
      " [  0   3 442  26   3  56   0]\n",
      " [  0   0  12 547   0   4   0]\n",
      " [  0  14   9   0 483   7   0]\n",
      " [  0   4  37   7   3 478   0]\n",
      " [ 18   2   0   0   0   0 512]]\n"
     ]
    }
   ],
   "source": [
    "confusion_rf = confusion_matrix(y_test, y_pred_rf, labels=[1,2,3,4,5,6,7])\n",
    "print(confusion_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check RF score on Kaggle\n",
    "\n",
    "- all_features: 0.85 on test, **0.7549** on Kaggle\n",
    "\n",
    "- top17_features: 0.84 on test, 0.73545 on Kaggle\n",
    "\n",
    "- top10_features: 0.826 on test, 0.70326 on Kaggle\n",
    "\n",
    "- top5_features: 0.7937 on test,  0.67985 on Kaggle\n",
    "\n",
    "- top2_features: 0.6045 on test, 0.49393 on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STEP 3: Train the KNN_pipe and Make predictions ########################\n",
    "rf_pipe.fit(data_train, data_train[target])\n",
    "\n",
    "pred = rf_pipe.predict(data_test)\n",
    "ID = data_test.Id\n",
    "\n",
    "# STEP 4: Write to file for Kaggle Submission #############################\n",
    "data_out = []\n",
    "for i in range(len(ID)):\n",
    "    data_out.append({'ID':ID.iloc[i], 'Cover_Type':pred[i]})\n",
    "\n",
    "import csv\n",
    "with open('test_submission_top2_rf_pipe.csv', 'wt') as fout:\n",
    "    cout = csv.DictWriter(fout, ['ID','Cover_Type'])\n",
    "    cout.writeheader()\n",
    "    cout.writerows(data_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SVM Classifier\n",
    "\n",
    "- simple LinearSVC(C=1, loss='hinge'): test score = 0.6325\n",
    "\n",
    "\n",
    "- Polynomial Kernel SVC(kernel=\"poly\", degree=3, coef0=1, C=5): \n",
    "\n",
    "- degree = 4: test score = 0.7829, Kaggle Score 0.66912\n",
    "- degree = 3: test score = 0.7688 \n",
    "- degree = 2: test score = 0.7481\n",
    "\n",
    "\n",
    "- Gaussian RBF Kernel\n",
    "\n",
    "- gamma=5, C=0.001: test score = 0.1452\n",
    "\n",
    "- gamma=5, C=1000: test score = 0.6663\n",
    "\n",
    "- gamma=0.1, C=0.001: test score = 0.1452\n",
    "\n",
    "- gamma=0.1, C=1000: test score = 0.8116, Kaggle Score **0.72226**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "svm_pipe = Pipeline(steps=[\n",
    "    ('preprocess', fu),\n",
    "    ('predict', LinearSVC(C=1, loss=\"hinge\"))\n",
    "])\n",
    "\n",
    "svm_pipe2 = Pipeline(steps=[\n",
    "    ('preprocess', fu),\n",
    "    ('predict', SVC(kernel=\"poly\", degree=4, coef0=1, C=5))\n",
    "])\n",
    "\n",
    "svm_pipe3 = Pipeline(steps=[\n",
    "    ('preprocess', fu),\n",
    "    ('predict', SVC(kernel=\"rbf\", gamma=0.1, C=1000))\n",
    "])\n",
    "\n",
    "\n",
    "svm_pipe2.fit(df_train, df_train[target])\n",
    "\n",
    "y_pred_svm = svm_pipe2.predict(df_test)  # get predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.71      0.68      0.70       552\n",
      "          2       0.72      0.59      0.65       561\n",
      "          3       0.73      0.69      0.71       530\n",
      "          4       0.88      0.96      0.92       563\n",
      "          5       0.79      0.89      0.83       513\n",
      "          6       0.75      0.76      0.75       529\n",
      "          7       0.89      0.93      0.91       532\n",
      "\n",
      "avg / total       0.78      0.78      0.78      3780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_svm = classification_report(y_test, y_pred_svm)\n",
    "print(report_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7763 0.7859 0.7945 0.7798 0.7779]\n",
      "0.7829\n"
     ]
    }
   ],
   "source": [
    "scores_svm = cross_val_score(svm_pipe2, df_train, df_train[target], cv=5)\n",
    "print(scores_svm.round(4))\n",
    "print(scores_svm.mean().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[404 103   0   0   6   1  38]\n",
      " [140 346   7   0  49  14   5]\n",
      " [  0  12 421  21   6  70   0]\n",
      " [  0   1  20 526   0  16   0]\n",
      " [  8  17  11   0 473   4   0]\n",
      " [  0  11  58   6   6 448   0]\n",
      " [ 21   7   0   0   0   0 504]]\n"
     ]
    }
   ],
   "source": [
    "confusion_svm = confusion_matrix(y_test, y_pred_svm, labels=[1,2,3,4,5,6,7])\n",
    "print(confusion_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STEP 3: Train the KNN_pipe and Make predictions ########################\n",
    "svm_pipe2.fit(data_train, data_train[target])\n",
    "\n",
    "pred = svm_pipe2.predict(data_test)\n",
    "ID = data_test.Id\n",
    "\n",
    "# STEP 4: Write to file for Kaggle Submission #############################\n",
    "data_out = []\n",
    "for i in range(len(ID)):\n",
    "    data_out.append({'ID':ID.iloc[i], 'Cover_Type':pred[i]})\n",
    "\n",
    "import csv\n",
    "with open('test_submission_polynomial_svm_deg4.csv', 'wt') as fout:\n",
    "    cout = csv.DictWriter(fout, ['ID','Cover_Type'])\n",
    "    cout.writeheader()\n",
    "    cout.writerows(data_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. AdaBoost Classifier\n",
    "\n",
    "- max_depth=6, n_estimators=200, learning_rate=0.5, test_score 0.74\n",
    "- max_depth=6, n_estimators=500, learning_rate=0.5, test_score 0.76\n",
    "- max_depth=10, n_estimators=500, learning_rate=0.5, test_score 0.8406, Kaggle score 0.75461\n",
    "- max_depth=15, n_estimators=500, learning_rate=0.5, test_score 0.854,  Kaggle score **0.77099**\n",
    "- max_depth=20, n_estimators=500, learning_rate=0.5, test_score 0.8549, Kaggle score 0.76533\n",
    "\n",
    "#### max=15, n_estimators=500, learning rate =0.854 seems to be the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ada_pipe = Pipeline(steps=[\n",
    "    ('preprocess', fu),\n",
    "    ('predict', AdaBoostClassifier(DecisionTreeClassifier(max_depth=15), \n",
    "                                   n_estimators=500, algorithm=\"SAMME.R\", learning_rate=0.5))\n",
    "])\n",
    "    \n",
    "ada_pipe.fit(df_train, df_train[target])\n",
    "\n",
    "y_pred_ada = ada_pipe.predict(df_test)  # get predicted labels   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.75      0.77      0.76       552\n",
      "          2       0.77      0.68      0.72       561\n",
      "          3       0.88      0.87      0.87       530\n",
      "          4       0.96      0.97      0.96       563\n",
      "          5       0.89      0.93      0.91       513\n",
      "          6       0.88      0.92      0.90       529\n",
      "          7       0.93      0.95      0.94       532\n",
      "\n",
      "avg / total       0.86      0.87      0.87      3780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_ada = classification_report(y_test, y_pred_ada)\n",
    "print(report_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8507 0.8604 0.8629 0.8526 0.8481]\n",
      "0.8549\n"
     ]
    }
   ],
   "source": [
    "scores_ada = cross_val_score(ada_pipe, df_train, df_train[target], cv=5)\n",
    "\n",
    "print(scores_ada.round(4))\n",
    "print(scores_ada.mean().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STEP 3: Train the KNN_pipe and Make predictions ########################\n",
    "ada_pipe.fit(data_train, data_train[target])\n",
    "\n",
    "pred = ada_pipe.predict(data_test)\n",
    "ID = data_test.Id\n",
    "\n",
    "# STEP 4: Write to file for Kaggle Submission #############################\n",
    "data_out = []\n",
    "for i in range(len(ID)):\n",
    "    data_out.append({'ID':ID.iloc[i], 'Cover_Type':pred[i]})\n",
    "\n",
    "import csv\n",
    "with open('test_submission_ada_maxdepth20_n500.csv', 'wt') as fout:\n",
    "    cout = csv.DictWriter(fout, ['ID','Cover_Type'])\n",
    "    cout.writeheader()\n",
    "    cout.writerows(data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Gradient Boosting Classifier\n",
    "\n",
    "## somehow not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(subsample=.7, n_estimators=100, max_depth=3, \n",
    "                                learning_rate=0.01,  random_state=3)\n",
    "\n",
    "gb.fit(df_train, df_train[target])\n",
    "\n",
    "y_pred_gb = gb.predict(df_test)  # get predicted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00       552\n",
      "          2       1.00      1.00      1.00       561\n",
      "          3       1.00      1.00      1.00       530\n",
      "          4       1.00      1.00      1.00       563\n",
      "          5       1.00      1.00      1.00       513\n",
      "          6       1.00      1.00      1.00       529\n",
      "          7       1.00      1.00      1.00       532\n",
      "\n",
      "avg / total       1.00      1.00      1.00      3780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_gb = classification_report(y_test, y_pred_gb)\n",
    "print(report_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cross-validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "scores_gb = cross_val_score(gb, df_train, df_train[target], cv=5)\n",
    "print(scores_gb.round(4))\n",
    "print(scores_gb.mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[552   0   0   0   0   0   0]\n",
      " [  0 561   0   0   0   0   0]\n",
      " [  0   0 530   0   0   0   0]\n",
      " [  0   0   0 563   0   0   0]\n",
      " [  0   0   0   0 513   0   0]\n",
      " [  0   0   0   0   0 529   0]\n",
      " [  0   0   0   0   0   0 532]]\n"
     ]
    }
   ],
   "source": [
    "confusion_gb = confusion_matrix(y_test, y_pred_gb, labels=[1,2,3,4,5,6,7])\n",
    "print(confusion_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameter_grid = {\n",
    "    'max_depth': range(1,6),\n",
    "    'learning_rate': [.01, .05, .1],\n",
    "    'max_features': [2, 5, 'auto']\n",
    "}\n",
    "\n",
    "param_searcher = GridSearchCV(gb, parameter_grid, cv=5)\n",
    "param_searcher.fit(df_train, df_train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_searcher.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_searcher.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GradientBoostingClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "vote_pipe = VotingClassifier(estimators =[('svm', svm_pipe), ('rf', rf_pipe), ('ada', ada_pipe)], \n",
    "                            voting = 'hard')\n",
    "\n",
    "vote_pipe.fit(df_train, df_train[target])\n",
    "\n",
    "y_pred_vote = vote_pipe.predict(df_test)  # get predicted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.74      0.76      0.75       552\n",
      "          2       0.79      0.64      0.70       561\n",
      "          3       0.88      0.83      0.85       530\n",
      "          4       0.95      0.98      0.96       563\n",
      "          5       0.88      0.93      0.91       513\n",
      "          6       0.84      0.91      0.87       529\n",
      "          7       0.91      0.96      0.94       532\n",
      "\n",
      "avg / total       0.85      0.86      0.85      3780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_vote = classification_report(y_test, y_pred_vote)\n",
    "print(report_vote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cross-validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8454 0.8493 0.8567 0.8473 0.8442]\n",
      "0.8486\n"
     ]
    }
   ],
   "source": [
    "scores_vote = cross_val_score(vote_pipe, df_train, df_train[target], cv=5)\n",
    "print(scores_vote.round(4))\n",
    "print(scores_vote.mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[419  75   0   0  10   2  46]\n",
      " [113 362   9   0  62  12   3]\n",
      " [  0   4 411  36   4  75   0]\n",
      " [  0   0  14 542   0   7   0]\n",
      " [  2  15  10   0 481   5   0]\n",
      " [  0   4  44  13   7 461   0]\n",
      " [ 19   2   0   0   0   0 511]]\n"
     ]
    }
   ],
   "source": [
    "confusion_vote = confusion_matrix(y_test, y_pred_vote, labels=[1,2,3,4,5,6,7])\n",
    "print(confusion_vote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Check Voting CLF score on Kaggle\n",
    "\n",
    "### all_features, (knn_pipe, lr_pipe, rf_pipe) for the vote_pipe\n",
    "\n",
    "- voting = \"hard\": test score is 0.831, Kaggle Score is 0.72975\n",
    "\n",
    "- voting = \"soft\": test sore is 0.8049, Kaggle Score is 0.69869 \n",
    "\n",
    "### all_features, (knn_pipe, svm_pipe, rf_pipe, ada_pipe) for the vote_pipe\n",
    "\n",
    "- voting = \"hard\": test score is 0.8482, Kaggle Score is 0.74582\n",
    "\n",
    "### all_features, (svm_pipe, rf_pipe, ada_pipe) for the vote_pipe\n",
    "\n",
    "- voting = \"hard\": test score is 0.8486, Kaggle Score is 0.74911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STEP 3: Train the KNN_pipe and Make predictions ########################\n",
    "vote_pipe.fit(data_train, data_train[target])\n",
    "\n",
    "pred = vote_pipe.predict(data_test)\n",
    "ID = data_test.Id\n",
    "\n",
    "# STEP 4: Write to file for Kaggle Submission #############################\n",
    "data_out = []\n",
    "for i in range(len(ID)):\n",
    "    data_out.append({'ID':ID.iloc[i], 'Cover_Type':pred[i]})\n",
    "\n",
    "import csv\n",
    "with open('test_submission_all_vote_pipe_svm_rf_ada_hard.csv', 'wt') as fout:\n",
    "    cout = csv.DictWriter(fout, ['ID','Cover_Type'])\n",
    "    cout.writeheader()\n",
    "    cout.writerows(data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
